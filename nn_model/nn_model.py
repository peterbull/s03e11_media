# AUTOGENERATED! DO NOT EDIT! File to edit: ../nn_model.ipynb.

# %% auto 0
__all__ = ['comp', 'path', 'df_train', 'df_test', 'df_comb', 'train_idxs', 'test_idxs', 'dep_var', 'procs', 'cont', 'cat',
           'splits', 'to_final', 'test_final', 'dls_final', 'nn_study', 'nn_lr', 'nn_train', 'epochs', 'learn_final',
           'test_dl', 'nn_preds', 'nn_trial']

# %% ../nn_model.ipynb 1
from fastai.tabular.all import *

from sklearn.model_selection import KFold, train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance

import xgboost as xgb

import seaborn as sns

import optuna

import json

# %% ../nn_model.ipynb 2
try: import fastkaggle
except ModuleNotFoundError:
    !pip install -Uq fastkaggle

from fastkaggle import *

# %% ../nn_model.ipynb 3
comp = 'playground-series-s3e11'
path = setup_comp(comp, install='fastai')

# %% ../nn_model.ipynb 4
df_train = pd.read_csv(path/'train.csv', low_memory=False)
df_test = pd.read_csv(path/'test.csv', low_memory=False)
df_comb = pd.concat([df_train, df_test], ignore_index=True)

# %% ../nn_model.ipynb 5
df_train.drop(['id'], axis=1, inplace=True)
df_test.drop(['id'], axis=1, inplace=True)
df_comb.drop(['id'], axis=1, inplace=True)

# %% ../nn_model.ipynb 6
df_train['store_sales_per_children'] = df_train['store_sales(in millions)'] / df_train['total_children']

# %% ../nn_model.ipynb 7
for column in df_train.columns:
    if (list(df_train[column].unique()) == [0.0, 1.0]):
        df_train.loc[:, column] = df_train[column].astype('bool')

# %% ../nn_model.ipynb 8
train_idxs = np.arange(len(df_train))
test_idxs = np.arange(len(df_train), len(df_comb))

# %% ../nn_model.ipynb 9
dep_var = 'cost'
procs = [Categorify, FillMissing, Normalize]
cont, cat = cont_cat_split(df_comb, max_card=1, dep_var=dep_var)
splits = RandomSplitter(valid_pct=0.2)(range_of(df_train))

# %% ../nn_model.ipynb 10
df_train = df_comb.iloc[train_idxs]
df_test = df_comb.iloc[test_idxs]

# %% ../nn_model.ipynb 11
to_final = TabularPandas(df_train, procs, cat, cont, y_names=dep_var, splits=splits)
test_final = TabularPandas(df_test, procs, cat, cont, y_names=None, splits=None)
dls_final = to_final.dataloaders(bs=1024)

# %% ../nn_model.ipynb 13
def nn_trial(trial):
    lr = trial.suggest_float('lr', 1e-5, 1e-1)
    wd = trial.suggest_float('wd', 1e-6, 1e-1)
    n_layers = trial.suggest_int('n_layers', 1, 3)
    hidden_dim = trial.suggest_int('hidden_dim', 100, 1000)
    

    layer_sizes = [hidden_dim] * n_layers

    learn = tabular_learner(dls_final, layers=layer_sizes, metrics=rmse, wd=wd)
    learn.fit_one_cycle(10, lr)

    # Return the validation loss (or any other metric of your choice)
    return learn.recorder.values[-1][0]


# %% ../nn_model.ipynb 14
nn_study = False

# %% ../nn_model.ipynb 15
if nn_study == True:
    study = optuna.create_study(direction='minimize')
    study.optimize(nn_trial, n_trials=50)
    with open('./training_params/nn_params.json', 'w') as fp:
        json.dump(study.best_params, fp)

# %% ../nn_model.ipynb 16
with open('./training_params/nn_params.json', 'r') as fp:
    nn_best_params = json.load(fp)

# %% ../nn_model.ipynb 17
nn_lr = nn_best_params['lr']
nn_best_params.pop('lr')

# %% ../nn_model.ipynb 18
nn_best_params['layers'] = [nn_best_params['hidden_dim']] * nn_best_params['n_layers']
nn_best_params.pop('n_layers')
nn_best_params.pop('hidden_dim')

# %% ../nn_model.ipynb 21
nn_train = False
epochs = 17

# %% ../nn_model.ipynb 22
if nn_train == True:
    learn_final = tabular_learner(dls_final, **nn_best_params, y_range=(0, 150), metrics=rmse)
    learn_final.fit_one_cycle(epochs, nn_lr)
    learn_final.export('models/tab_learner.pkl')


# %% ../nn_model.ipynb 23
learn_final = load_learner('models/tab_learner.pkl')

# %% ../nn_model.ipynb 25
test_dl = learn_final.dls.test_dl(df_test)
nn_preds, _ = learn_final.get_preds(dl=test_dl)

# %% ../nn_model.ipynb 26
nn_preds = nn_preds.squeeze().numpy()
np.savetxt('./predictions/nn_preds.csv', nn_preds, delimiter=',')

# %% ../nn_model.ipynb 27
nn_preds = np.loadtxt('./predictions/nn_preds.csv', delimiter=',')
