# AUTOGENERATED! DO NOT EDIT! File to edit: ../xgb_model.ipynb.

# %% auto 0
__all__ = ['comp', 'path', 'df_train', 'df_test', 'df_comb', 'train_idxs', 'test_idxs', 'dep_var', 'procs', 'cont', 'cat',
           'splits', 'to_final', 'test_final', 'dls_final', 'train_fi', 'target_fi', 'x_train', 'x_test', 'y_train',
           'y_test', 'fi_params', 'model', 'r', 'fi', 'features', 'cols_to_drop', 'train_fe', 'test_fe', 'train',
           'target', 'd_train', 'd_test', 'glob_params', 'run_study', 'best_params', 'run_max_boost', 'num_boost_round',
           'to', 'to_test', 'xs', 'y', 'valid_xs', 'valid_y', 'test_xs', 'k_folds_train', 'plot_fi', 'fe', 'objective',
           'max_boost_round']

# %% ../xgb_model.ipynb 1
from fastai.tabular.all import *

from sklearn.model_selection import KFold, train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance


import xgboost as xgb

import seaborn as sns

import optuna

import json

# %% ../xgb_model.ipynb 2
try: import fastkaggle
except ModuleNotFoundError:
    !pip install -Uq fastkaggle

from fastkaggle import *

# %% ../xgb_model.ipynb 3
comp = 'playground-series-s3e11'
path = setup_comp(comp, install='fastai')

# %% ../xgb_model.ipynb 4
df_train = pd.read_csv(path/'train.csv', low_memory=False)
df_test = pd.read_csv(path/'test.csv', low_memory=False)
df_comb = pd.concat([df_train, df_test], ignore_index=True)

# %% ../xgb_model.ipynb 5
df_train.drop(['id'], axis=1, inplace=True)
df_test.drop(['id'], axis=1, inplace=True)
df_comb.drop(['id'], axis=1, inplace=True)

# %% ../xgb_model.ipynb 6
df_train['store_sales_per_children'] = df_train['store_sales(in millions)'] / df_train['total_children']

# %% ../xgb_model.ipynb 7
for column in df_train.columns:
    if (list(df_train[column].unique()) == [0.0, 1.0]):
        df_train.loc[:, column] = df_train[column].astype('bool')

# %% ../xgb_model.ipynb 8
train_idxs = np.arange(len(df_train))
test_idxs = np.arange(len(df_train), len(df_comb))

# %% ../xgb_model.ipynb 9
dep_var = 'cost'
procs = [Categorify, FillMissing, Normalize]
cont, cat = cont_cat_split(df_comb, max_card=1, dep_var=dep_var)
splits = RandomSplitter(valid_pct=0.2)(range_of(df_train))

# %% ../xgb_model.ipynb 10
df_train = df_comb.iloc[train_idxs]
df_test = df_comb.iloc[test_idxs]

# %% ../xgb_model.ipynb 11
to_final = TabularPandas(df_train, procs, cat, cont, y_names=dep_var, splits=splits)
test_final = TabularPandas(df_test, procs, cat, cont, y_names=None, splits=None)
dls_final = to_final.dataloaders(bs=1024)

# %% ../xgb_model.ipynb 12
train_fi = df_train.drop(columns = ['cost'])
target_fi = df_train['cost']

# %% ../xgb_model.ipynb 13
def plot_fi(data,ax = None, title = None):
    fi = pd.Series(data, index = train_fi.columns).sort_values(ascending = True)
    fi.plot(kind = 'barh', ax = ax)

# %% ../xgb_model.ipynb 14
x_train, x_test, y_train, y_test = train_test_split(train_fi, target_fi, test_size=0.2)

# %% ../xgb_model.ipynb 15
fi_params = {'learning_rate': 0.2456172216493356,
 'max_depth': 10,
 'lambda': 0.0023120639864473262,
 'alpha': 0.5848465230832824,
 'colsample_bytree': 0.9966638720347625,
 'min_child_weight': 0,
 'objective': 'reg:squaredlogerror',
 'eval_metric': 'rmsle'}

# %% ../xgb_model.ipynb 16
model = xgb.XGBRegressor(**fi_params)


# %% ../xgb_model.ipynb 17
model.fit(x_train, y_train)

# %% ../xgb_model.ipynb 19
r = permutation_importance(model, x_test, y_test, n_repeats=1, random_state=46)

# %% ../xgb_model.ipynb 20
fi = pd.Series(r['importances'].reshape(15,), index = train_fi.columns).sort_values(ascending = True)

# %% ../xgb_model.ipynb 22
features = list(fi.index)
cols_to_drop = features[0:6]

# %% ../xgb_model.ipynb 23
df_train = df_train.drop(columns=cols_to_drop)

# %% ../xgb_model.ipynb 24
df_test = df_test.drop(columns=cols_to_drop)

# %% ../xgb_model.ipynb 25
def fe(data):
    data = data.replace([np.inf, -np.inf], 10)
    return data

# %% ../xgb_model.ipynb 26
train_fe = fe(df_train)
test_fe = fe(df_test)

# %% ../xgb_model.ipynb 27
train = train_fe.drop(columns=['cost'])

# %% ../xgb_model.ipynb 28
target = train_fe['cost']

# %% ../xgb_model.ipynb 30
x_train, x_test, y_train, y_test = train_test_split(train,target, test_size = 0.2)
d_train = xgb.DMatrix(x_train,y_train)
d_test = xgb.DMatrix(x_test,y_test)


# %% ../xgb_model.ipynb 31
glob_params = {
    'objective': 'reg:squaredlogerror',
    'eval_metric': 'rmsle'
}

# %% ../xgb_model.ipynb 32
def objective(trial):
    params = {
        'verbosity':0,
        'learning_rate': trial.suggest_float('learning_rate',1e-10,1.0),
        'max_depth':trial.suggest_int('max_depth', 3, 12),
#        'gamma':trial.suggest_float('gamma',0,8),
        'lambda': trial.suggest_float('lambda', 1e-8, 1.0),
        'alpha': trial.suggest_float('alpha', 1e-8, 1.0),
        'colsample_bytree':trial.suggest_float('colsample_bytree',1e-5,1.0),
        'min_child_weight':trial.suggest_int('min_child_weight',0,1),
        'booster':trial.suggest_categorical("booster", ["dart", "gbtree",'gblinear']),
        'sampling_method': trial.suggest_categorical('sampling_method',['uniform','gradient_based']),
        'grow_policy': trial.suggest_categorical('grow_policy',['depthwise','lossguide']),
#         'reg_alpha': trial.suggest_float("reg_alpha", 0, 1),
#         'reg_lambda': trial.suggest_float("reg_lambda", 0, 1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1500),    
    }
    num_boost_round = params.pop('num_boost_round')
    params.update(glob_params)
    sch_prm = xgb.train(params, d_train, num_boost_round=200,
                        evals = [(d_test, 'valid')], 
                        early_stopping_rounds=20, verbose_eval=False
                       )
    
    return sch_prm.best_score


# %% ../xgb_model.ipynb 33
run_study = False

# %% ../xgb_model.ipynb 34
if run_study == True:
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials = 100,)
    # save study.best_params a json file to load later
    with open('./training_params/xgb_params.json', 'w') as fp:
        json.dump(study.best_params, fp)

# %% ../xgb_model.ipynb 36
with open('./training_params/xgb_params.json', 'r') as fp:
    study_best_params = json.load(fp)

# %% ../xgb_model.ipynb 38
best_params = {
 'learning_rate': 0.9319687783879956,
 'max_depth': 12,
 'lambda': 0.02022953533538882,
 'alpha': 9.277352577569814e-07,
 'colsample_bytree': 0.7477567097593131,
 'min_child_weight': 0,
 'booster': 'gbtree',
 'sampling_method': 'uniform',
 'grow_policy': 'depthwise',
 'objective': 'reg:squaredlogerror',
 'eval_metric': 'rmsle'}

# %% ../xgb_model.ipynb 39
best_params.update(study_best_params)

# %% ../xgb_model.ipynb 40
def max_boost_round(trial):
    params = {
        'num_boost_round': trial.suggest_int('num_boost_round', 100, 1500),
    }
    num_boost_round = params.pop('num_boost_round')
    params.update(glob_params)
    sch_prm = xgb.train(params, d_train, num_boost_round=num_boost_round,
                        evals = [(d_test, 'valid')], 
                        early_stopping_rounds=20, verbose_eval=False
                       )
    
    return sch_prm.best_score

# %% ../xgb_model.ipynb 41
run_max_boost = False

# %% ../xgb_model.ipynb 42
if run_max_boost == True:
    study = optuna.create_study(direction='minimize')
    study.optimize(max_boost_round, n_trials = 20,)
    # save study.best_params a json file to load later
    with open('./training_params/boost_params.json', 'w') as fp:
        json.dump(study.best_params, fp)

# %% ../xgb_model.ipynb 43
with open('./training_params/boost_params.json', 'r') as fp:
    boost_best_params = json.load(fp)

# %% ../xgb_model.ipynb 44
num_boost_round = boost_best_params['num_boost_round']

# %% ../xgb_model.ipynb 47
cont, cat = cont_cat_split(df_train, max_card=1, dep_var='cost')

# %% ../xgb_model.ipynb 48
to = TabularPandas(df_train, procs, cat, cont, y_names='cost', splits=splits)
to_test = TabularPandas(df_test, procs, cat, cont, y_names=None, splits=None)

# %% ../xgb_model.ipynb 49
xs, y = to.train.xs, to.train.y
valid_xs, valid_y = to.valid.xs, to.valid.y
test_xs = to_test.xs

# %% ../xgb_model.ipynb 59
k_folds_train = False

# %% ../xgb_model.ipynb 60
if k_folds_train == True:
    # set the number of folds
    num_folds = 10
    # create empty list to store the loglosses
    loglosses = []
    # create empty array to store the predictions
    k_gb_preds = np.zeros(len(to_test))
    # create a kfold object
    kf = KFold(n_splits=num_folds, shuffle = True, random_state = 1042)
    # loop through each fold
    for fold, (trn_idx, val_idx) in enumerate(kf.split(X=xs, y=y)):
        # print the fold number
        print('-'*20, 'Fold:', fold + 1, '-'*20)
        # split the data into train and validation
        X_train, X_test = train.iloc[trn_idx], train.iloc[val_idx]
        # split the target into train and validation
        y_train, y_test = target.iloc[trn_idx], target.iloc[val_idx]
        
        # create the train and validation data
        d_train = xgb.DMatrix(X_train,y_train)
        d_test = xgb.DMatrix(X_test,y_test)
        
        # train the model
        k_gb_model =  xgb.train(best_params, d_train, num_boost_round = num_boost_round,
                            evals = [(d_test, 'valid')], 
                            early_stopping_rounds=20, verbose_eval=True
                        )
    #     preds = model.predict(d_test)
    #     logloss = metrics.log_loss(y_test, preds)
    #     loglosses.append(logloss)
    #     print(f'Logloss: {logloss}')
        # predict on the test set
        preds = k_gb_model.predict(xgb.DMatrix(test_xs))
        # add the predictions to the final predictions array
        k_gb_preds += preds / num_folds

    # save k_gb_preds to csv
    np.savetxt('./predictions/k_gb_preds.csv', k_gb_preds, delimiter=',')

# %% ../xgb_model.ipynb 61
if 'k_gb_preds' not in locals():
    k_gb_preds = np.loadtxt('./predictions/k_gb_preds.csv', delimiter=',')
